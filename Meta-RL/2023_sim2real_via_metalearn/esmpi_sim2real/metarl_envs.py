"""
Adapted from https://github.com/tristandeleu/pytorch-maml-rl,
which is in turn adapted from https://github.com/cbfinn/maml_rl .
"""

## TODO: adapt the code for hopper wrappers task and sample in particular 


from abc import ABC
import numpy as np
from copy import copy

from gymnasium.envs.mujoco.half_cheetah_v4 import HalfCheetahEnv
from gymnasium.envs.mujoco.hopper_v4 import HopperEnv



class MetaRLEnv(ABC):
    def sample_task(self):
        """
        Return a randomly sampled task descriptor (as Python dictionary).
        """
        pass

    def set_task(self, task):
        """
        Set the environment to the new task defined by task descriptor `task` (dictionary).
        """
        pass


class HalfCheetahVelEnv(HalfCheetahEnv, MetaRLEnv):
    """
    Half-cheetah environment with target velocity, as described in [1]. The
    code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand.py

    The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
    time step a reward composed of a control cost and a penalty equal to the
    difference between its current velocity and the target velocity. The tasks
    are generated by sampling the target velocities from the uniform
    distribution on [0, 2].

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
        model-based control", 2012
        (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
    """
    def __init__(self, **kwargs):
        self.low = 0.0
        self.high = 2.0

        self._goal_vel = 1.0
        super(HalfCheetahVelEnv, self).__init__()

    def step(self, action):
        x_position_before = self.data.qpos[0]
        self.do_simulation(action, self.frame_skip)
        x_position_after = self.data.qpos[0]

        #x_velocity = self.data.qvel[0]
        x_velocity = (x_position_after - x_position_before) / self.dt

        forward_reward = -1.0 * abs(x_velocity - self._goal_vel)
        ctrl_cost = self.control_cost(action)

        observation = self._get_obs()
        reward = forward_reward ###- ctrl_cost
        terminated = False
        info = {
            "x_position": x_position_after,
            "x_velocity": x_velocity,
            "reward_run": forward_reward,
            "reward_ctrl": -ctrl_cost,
            "task": self._task
        }

        if self.render_mode == "human":
            self.render()
        return observation, reward, terminated, False, info

    def get_task(self):
        return self._task

    def sample_task(self):
        velocity = self.np_random.uniform(self.low, self.high)
        task = {'velocity': velocity}
        return task

    def set_task(self, task):
        self._task = copy(task)
        self._goal_vel = task.get('velocity')


class HalfCheetahDirEnv(HalfCheetahEnv, MetaRLEnv):
    """Half-cheetah environment with target direction, as described in [1]. The
    code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand_direc.py

    The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
    time step a reward composed of a control cost and a reward equal to its
    velocity in the target direction. The tasks are generated by sampling the
    target directions from a Bernoulli distribution on {-1, 1} with parameter
    0.5 (-1: backward, +1: forward).

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
        model-based control", 2012
        (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
    """
    def __init__(self, **kwargs):
        self._task = {}
        self._goal_dir = 1
        super(HalfCheetahDirEnv, self).__init__(**kwargs)

    def step(self, action):
        x_position_before = self.data.qpos[0]
        self.do_simulation(action, self.frame_skip)
        x_position_after = self.data.qpos[0]

        #x_velocity = self.data.qvel[0]
        x_velocity = (x_position_after - x_position_before) / self.dt

        ctrl_cost = self.control_cost(action)

        forward_reward = self._goal_dir * x_velocity

        observation = self._get_obs()
        reward = forward_reward ###- ctrl_cost
        terminated = False
        info = {
            "x_position": x_position_after,
            "x_velocity": x_velocity,
            "reward_run": forward_reward,
            "reward_ctrl": -ctrl_cost,
            "task": self._task
        }

        if self.render_mode == "human":
            self.render()
        return observation, reward, terminated, False, info

    def get_task(self):
        return self._task

    def sample_task(self):
        direction = 2 * self.np_random.binomial(1, p=0.5) - 1
        task = {'direction': direction}
        return task

    def set_task(self, task):
        self._task = task
        self._goal_dir = task.get('direction')


